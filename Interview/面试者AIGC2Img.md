### 一、自我介绍+技术背景摸底（10分钟）
#### 1. 3分钟自我介绍  
“面试官您好！我本科是计算机科学与技术专业，有4年后端开发经验，其中2年聚焦AIGC生图/生视频方向，目前在一家互联网公司担任AIGC后端技术负责人。近2年核心项目是「企业级生视频智能生成平台」，负责从0到1搭建后端架构，支撑电商、广告行业客户的视频自动化生成需求。

我的核心贡献包括：一是基于Python+FastAPI搭建高并发服务，通过协程优化、任务队列调度，将单GPU生视频QPS从8提升至35，延迟从12秒降至4.5秒；二是主导模型工程化落地，通过ONNX+TensorRT优化，将Sora同类生视频模型推理速度提升60%，显存占用降低35%；三是设计多存储方案，支撑日均10万+生成任务，数据存储成本降低20%。

技术栈上，我精通Python异步开发（FastAPI/asyncio）、AIGC生态库（Diffusers/OpenCV），熟悉TensorRT/ONNX Runtime推理加速，了解Go底层原理及GRPC跨语言通信，能独立完成AIGC服务从架构设计、开发部署到监控运维的全流程。”

#### 2. 岗位核心3个技术要求+自身优劣势  
“我认为这个岗位最核心的3个技术要求是：①Python高并发服务开发与性能优化（支撑AIGC耗时任务的高可用）；②AIGC模型工程化部署与推理加速（解决模型落地的延迟、显存问题）；③多存储与监控体系设计（保障数据安全与服务可观测性）。

我的优势：一是有完整的生图/生视频项目落地经验，踩过大量实战坑（如高并发调度、模型显存溢出、跨语言通信超时），能快速定位解决问题；二是技术栈与岗位高度匹配，Python异步、模型加速、存储监控均有成熟落地案例，且熟悉AIGC业务场景的核心痛点。

不足：Go语言的开发经验集中在跨语言协作和简单模块优化（如GRPC通信、任务调度适配），独立开发复杂Go底层模块的经验较少，但我已系统学习GPM调度、内存分配原理，近期正在参与公司Go语言任务调度模块的二次开发，弥补这一短板。”

#### 3. 常用工具及选型理由  
“Python后端框架优先用FastAPI，因为它原生支持异步、性能接近Go，且自动生成接口文档，适配AIGC高并发场景；AIGC相关库常用Diffusers（加载生图/生视频模型）、OpenCV/moviepy（结果后处理）、torch（模型推理辅助）；性能优化工具用memory_profiler（内存排查）、cProfile（性能分析）、火焰图（链路耗时定位）；任务队列用Celery+Redis，因为生态成熟、支持优先级调度和重试机制，适配生成任务的异步化需求。

选型核心逻辑是「场景适配+效率优先」：AIGC服务的核心痛点是高并发、低延迟，所以工具选择优先考虑异步支持、性能表现，同时兼顾生态成熟度和运维成本，避免小众工具带来的踩坑风险。”


### 二、核心技术深度考察（30分钟）
#### （1）Python高并发与性能优化  
##### 问题1：Python协程（asyncio）工作原理+与多进程/多线程的适用场景区别+AIGC场景选择  
“Python协程的核心是「用户态轻量级线程」，基于事件循环（Event Loop）实现，通过yield from/await关键字切换任务，避免内核态线程切换的开销。其工作原理是：事件循环管理所有协程任务，当某个协程遇到IO阻塞（如网络请求、数据库查询）时，自动切换到其他就绪协程，直到阻塞任务完成，再恢复执行，从而提升CPU利用率。

与多进程、多线程的适用场景区别：①多进程：适合CPU密集型任务（如模型推理），因为能突破GIL限制，利用多核CPU；②多线程：适合IO密集型任务，但受GIL影响，多线程在CPU密集场景下效率低；③协程：适合高并发IO密集型任务（如请求接收、数据存储、模型调用的网络通信），开销远低于多线程，单进程可支持上万协程。

AIGC场景的选择：我会采用「协程+多进程」混合架构。比如生视频服务中，用FastAPI+asyncio处理高并发请求（IO密集），接收请求后通过多进程池（multiprocessing.Pool）调用模型推理（CPU/GPU密集），模型推理完成后，再用协程异步写入存储。这样既利用协程的高并发处理能力，又通过多进程突破GIL限制，提升模型推理的并行度。”

##### 问题2：Python服务性能调优的典型问题+排查解决+工具  
“我遇到过3类典型问题，均在生视频项目中落地解决：

①内存泄漏：现象是服务运行一段时间后，内存占用持续上升，最终OOM。排查工具用memory_profiler+objgraph，通过装饰器标记核心函数，监控内存变化，定位泄漏点。比如曾发现Diffusers模型生成的中间张量（latents）未及时释放，导致每次推理内存增加1-2GB。解决方式：在推理完成后，手动调用torch.cuda.empty_cache()释放显存，同时用对象池复用大内存对象（如固定尺寸的张量），最终内存占用稳定在合理范围，泄漏问题彻底解决。

②CPU占用高：现象是QPS不高，但CPU使用率超80%。排查工具用cProfile+火焰图，分析函数耗时占比。发现是生视频帧拼接时，用了同步的moviepy接口，且未批量处理。解决方式：改用asyncio封装的异步视频处理库，同时实现帧批量拼接（每10帧批量处理一次），CPU使用率降至30%以下。

③QPS上不去：现象是并发请求增加时，QPS停滞不前，响应延迟飙升。排查工具用Prometheus+Grafana监控接口耗时，发现是模型调用未做并发控制，导致线程池阻塞。解决方式：用asyncio.Semaphore限制并发模型调用数（如单GPU限制20个并发），同时引入Celery任务队列，实现请求排队、优先级调度（付费用户优先级设为1，普通用户设为3），QPS从10提升至35，延迟从12秒降至4.5秒。”

##### 问题3：生图/生视频请求排队机制设计+任务优先级/重试/限流+Celery优缺点  
“请求排队机制设计：采用「前端限流+中间件排队+后端调度」三层架构。①前端限流：用Nginx配置limit_req模块，限制单IP每秒请求数，避免恶意压测；②中间件排队：用Celery+Redis作为任务队列，接收请求后生成唯一任务ID，返回给用户，用户可通过ID查询状态；③后端调度：基于GPU负载动态分配任务，空闲GPU优先处理高优先级任务。

任务优先级：通过Celery的task_priority参数实现，分为3级（1-3），付费用户任务设为1，普通用户设为3，紧急业务（如客户加急订单）设为2。同时在Worker端配置--priority_queue参数，优先消费高优先级队列。

任务重试：在Celery任务装饰器中设置retry_backoff=2（指数退避）、retry_kwargs={'max_retries':3}，当模型调用失败（如显存不足、网络超时）时，自动重试，且重试间隔逐渐延长（1秒→2秒→4秒），避免频繁重试导致服务过载。

限流：除了前端Nginx限流，后端用Redis实现分布式限流（基于令牌桶算法），设置全局QPS上限（如单服务100QPS），当超出阈值时，返回「请求繁忙，请稍后重试」的友好提示。

Celery的优缺点：优点是生态成熟、支持优先级调度、重试机制、分布式部署，且与Python生态兼容性好；缺点是配置较复杂，跨语言支持弱（若后端有Go模块，需额外适配），且任务队列堆积时，监控和排查成本较高。我们的优化方式是：结合Prometheus监控Celery队列长度，设置告警阈值（如队列长度超500触发告警），同时定期清理过期任务，避免队列膨胀。”

##### 问题4：Diffusers库使用+Stable Diffusion模型显存占用和推理速度优化  
“我常用Diffusers加载Stable Diffusion的Text-to-Image、Image-to-Image模型，核心流程是：通过DiffusionPipeline.from_pretrained()加载模型，设置device（GPU/CPU）、dtype（FP16/FP32），传入prompt和生成参数（steps、CFG Scale、分辨率），调用pipe()方法生成图片。

优化显存占用的3个关键手段：①精度量化：将模型 dtype 从FP32改为FP16，显存占用直接减半（如Stable Diffusion 1.5从4GB降至2GB），若对精度要求不高，可尝试INT8量化（需配合TensorRT）；②模型分片：用device_map="auto"参数，让Diffusers自动将模型层分配到CPU和GPU，仅推理时将当前层加载到GPU，推理完成后释放，适合显存较小的GPU（如8GB显存）；③关闭不必要的功能：如禁用模型的attention slicing、gradient checkpointing，减少额外显存开销。

推理速度优化：①批量处理：将多个生成请求合并为一个batch，调用模型推理（如batch size=8），提升GPU利用率，单张图片推理时间从300ms降至120ms；②算子融合：用ONNX将模型转换为ONNX格式，再通过TensorRT进行算子融合（如将Conv+BN+Relu融合为一个算子），减少GPU kernel调用次数；③预热模型：服务启动时，提前加载常用模型到显存，避免首次请求的模型加载耗时（从5秒降至500ms）。”

#### （2）AIGC模型工程化部署  
##### 问题1：生图模型从算法交付到线上服务的完整流程+技术环节  
“完整流程分为6个核心环节，我以Stable Diffusion生图模型为例说明：

①模型接收与验证：算法团队交付PyTorch格式的模型文件（.bin/.ckpt）、配置文件（config.json）、依赖库清单，我会先在测试环境加载模型，验证生成效果（如输入prompt是否能生成符合预期的图片）、输入输出格式是否标准化（如prompt长度限制、图片分辨率支持范围）。

②模型格式转换：将PyTorch模型转换为ONNX格式（用torch.onnx.export()），统一模型接口，方便后续推理框架集成；若需进一步优化，会转换为TensorRT引擎文件（.trt），适配GPU加速。

③推理框架集成：选择TensorRT作为推理加速框架，配置量化策略（FP16）、 batch size 范围（1-16），生成优化后的推理引擎；同时封装推理接口，支持动态传入生成参数（steps、CFG Scale、风格参数）。

④服务接口开发：用FastAPI封装推理接口，支持同步查询（短耗时生图任务）和异步回调（长耗时任务），接口包含参数校验（如分辨率合法性、prompt长度限制）、任务ID生成、结果存储路径返回。

⑤部署与扩容：将服务容器化（Docker），通过K8s部署，配置GPU亲和性调度（根据GPU型号和负载分配Pod）；设置HPA（Horizontal Pod Autoscaler），根据QPS和GPU利用率自动扩容（最小3个Pod，最大10个Pod）。

⑥监控与灰度发布：上线前先灰度发布（5%流量），通过Prometheus监控QPS、延迟、模型调用成功率、GPU使用率；灰度期间无异常后，逐步放大流量至100%；同时设置告警（如失败率超1%、延迟超5秒触发通知）。”

##### 问题2：TensorRT/ONNX Runtime推理加速实战+效果  
“我主要用TensorRT做生图/生视频模型的推理加速，具体流程：

①模型转换：先将PyTorch模型转ONNX，需注意指定动态维度（如batch size、图片高度/宽度），避免后续推理时维度固定导致的兼容性问题；然后用TensorRT的trtexec工具将ONNX模型转换为TensorRT引擎，配置--fp16参数开启半精度量化，同时启用--saveEngine保存引擎文件（避免重复转换）。

②优化手段：除了量化，还会开启TensorRT的算子融合（如Conv+BN+Scale融合）、层融合（将连续的卷积层融合为一个块），减少GPU kernel的调用次数；针对Transformer结构的模型（如Stable Diffusion的UNet），启用TensorRT的FlashAttention优化，提升注意力机制的计算效率。

③实际效果：以Stable Diffusion 1.5生图模型为例，未优化前FP32精度推理，单张512×512图片耗时300ms，显存占用4GB；经TensorRT FP16量化+算子融合后，推理耗时降至80ms（提升73%），显存占用降至2GB（降低50%）；生视频模型（Sora同类，输入文本生成10秒1080P视频）未优化前推理耗时15秒，优化后降至5.5秒（提升63%），单GPU并发支持从3提升至10。

ONNX Runtime我主要用于CPU推理场景（如低配置服务器），通过开启ONNX Runtime的CPU多线程（设置intra_op_num_threads=8）和量化优化，推理速度比原生PyTorch提升40%左右，适合对延迟要求不高的轻量场景。”

##### 问题3：生视频模型部署方案（低延迟+高可用）+多GPU调度/模型并行选择  
“生视频模型的核心痛点是参数量大（通常10B+）、推理耗时久（10秒+）、显存占用高（16GB+），部署方案设计如下：

①低延迟优化：采用「模型并行+张量并行」混合并行策略。模型并行：将生视频模型的不同层拆分到多个GPU（如编码器在GPU 0，解码器在GPU 1），避免单GPU显存不足；张量并行：将模型层的权重拆分到多个GPU（如将Conv层的权重拆分为2份，分别在GPU 0和GPU 1计算），提升计算并行度。同时结合TensorRT FP16量化、动态批处理（Batch Size根据GPU负载自适应调整，1-8），进一步降低延迟。

②高可用保障：a. 多副本部署：每个模型部署3个以上Pod，分布在不同节点，避免单点故障；b. 模型热备：加载常用模型到显存，同时在磁盘缓存备用模型，当主模型故障时，1秒内切换到备用模型；c. 任务重试与降级：模型调用失败时，自动重试2次，重试失败则降级为低精度模型（如从1080P降至720P），保障服务可用性；d. 熔断机制：当某个Pod的失败率超5%，自动将其从服务集群中剔除，避免影响整体服务。

③多GPU调度：基于K8s的GPU调度插件（如NVIDIA GPU Operator），监控每个GPU的使用率、显存占用、温度，将新任务分配给负载最低的GPU；同时支持GPU共享（通过MPS技术），让多个轻量任务共享同一GPU，提升GPU利用率（从原来的40%提升至75%）。

模型并行与张量并行的选择：当模型单卡显存不足时，优先用模型并行（拆分层）；当模型单卡可容纳，但计算耗时久时，优先用张量并行（拆分权重）；生视频模型通常两者结合，既解决显存问题，又提升计算速度。”

#### （3）Go底层与跨语言协同  
##### 问题1：Go的GPM调度模型+与Python协程调度的本质区别+对AIGC服务性能的影响  
“Go的GPM调度模型由G（Goroutine）、P（Processor）、M（Machine）三部分组成：G是轻量级协程（内存占用2KB），M是操作系统内核线程，P是逻辑处理器（关联M和G的调度上下文）。核心原理是：P维护一个本地G队列，M通过P获取G并执行；当G遇到阻塞（如系统调用、channel操作），P会将其从M上剥离，同时从全局队列或其他P的本地队列获取新G执行，避免M空闲，实现高效调度。

与Python协程调度的本质区别：①调度层面：Go的GPM是「内核态+用户态」协同调度，M对应内核线程，P负责G的调度，可利用多核；Python的asyncio是「用户态调度」，依赖事件循环，所有协程运行在单个内核线程上，受GIL限制，无法利用多核；②切换开销：Go的Goroutine切换是内核级优化，开销远低于Python协程；③并发能力：Go单进程可支持百万级Goroutine，Python协程受内存和调度效率限制，单进程通常支持上万级。

对AIGC服务性能的影响：AIGC服务的核心是高并发请求处理和模型推理并行度。Go的GPM调度模型适合处理高并发任务调度（如GPU资源管理、任务分发），因为其调度高效、可利用多核，能提升任务调度的吞吐量；而Python协程适合处理IO密集型的请求接收和数据存储，但受GIL限制，无法高效处理CPU密集型的模型推理。因此，采用「Python（业务层）+ Go（底层调度层）」的架构，可互补优势：Python处理高并发IO，Go负责高效任务调度和资源管理，整体提升服务的并发能力和资源利用率。”

##### 问题2：Python与Go跨语言通信+GRPC使用+Protobuf优势+问题解决  
“Python与Go跨语言通信的核心方案是GRPC，因为它基于HTTP/2协议，支持双向流、序列化效率高、支持多语言，适合高性能服务通信。

具体实现流程：①定义Protobuf协议文件（.proto），声明服务接口和数据结构（如任务请求、任务结果）；②用protoc编译器生成Python和Go的代码（Python用grpcio-tools，Go用protoc-gen-go-grpc）；③Go端实现服务接口（如任务调度、GPU资源查询），启动GRPC服务；④Python端通过生成的客户端代码，调用Go端的GRPC接口，传递任务数据。

Protobuf的序列化优势：①效率高：二进制序列化，比JSON体积小30%-50%，传输速度快；②跨语言兼容性强：支持Python、Go、Java等多种语言，生成的代码可直接调用，避免手动解析数据；③扩展性好：支持字段新增和默认值，兼容旧版本服务，无需修改现有代码。

常见问题及解决：①超时问题：跨语言调用时，网络延迟或Go端处理耗时过长，导致Python端超时。解决方式：在GRPC客户端设置超时时间（如5秒），同时实现重试机制（指数退避）；Go端优化处理逻辑，避免阻塞，若处理耗时久，采用异步回调模式，先返回任务ID，后续通过另一个接口查询结果。②数据不一致：Protobuf字段类型定义错误（如Python的int对应Go的uint），导致数据解析异常。解决方式：严格校验Protobuf协议文件的字段类型，开发环境中添加数据校验逻辑，同时在测试环境进行全链路联调，确保数据传输正确。③连接池耗尽：Python端频繁创建GRPC客户端连接，导致连接池耗尽。解决方式：Python端使用GRPC的连接池（如grpc.experimental.channel_pool），复用连接，减少连接创建销毁的开销。”

##### 问题3：岗位用Go做底层优化的原因+适合Go实现的模块+理由  
“岗位用Go做底层优化的核心原因：①Go的GPM调度模型适合高并发任务调度，能提升AIGC服务的任务分发和资源管理效率；②Go的内存管理（TCMalloc）和垃圾回收（三色标记+写屏障）优化较好，内存占用低、GC停顿短，适合长期运行的底层服务；③Go的跨语言支持好，可与Python协同，互补各自优势（Python擅长AI生态，Go擅长高性能调度）；④Go的编译型特性，执行效率高于Python，适合对性能要求高的底层模块。

适合Go实现的模块及理由：①GPU资源调度模块：需处理高并发的任务分发和GPU负载监控，Go的GPM调度能高效利用多核，提升调度吞吐量，且Go的并发编程模型比Python更简洁；②任务队列管理模块：需支持高并发的任务入队、出队、优先级排序，Go的channel和goroutine可实现高效的队列操作，且性能优于Python的Celery；③跨服务通信网关：需处理大量跨语言（Python/Go）的请求转发，Go的GRPC服务性能高、支持双向流，适合作为网关层；④底层监控模块：需实时采集服务指标（如GPU使用率、任务状态），Go的轻量级特性和高效的系统调用能力，能减少监控模块对业务服务的资源占用。”

#### （4）存储与监控体系  
##### 问题1：生图/生视频存储方案选择+理由+读写性能和成本优化  
“存储方案采用「分层存储」策略，根据数据类型和访问频率选择不同存储引擎：

①任务元数据（如任务ID、用户ID、生成参数、状态、创建时间）：选择PostgreSQL。理由：元数据是结构化数据，需支持复杂查询（如按用户ID查询历史任务、按状态统计任务数量），PostgreSQL的索引优化（如B树索引、GIN索引）和事务支持，能保障查询效率和数据一致性。优化：按任务创建时间分表（每月一张表），避免单表数据量过大（超1000万条）导致的查询变慢；对常用查询字段（任务ID、用户ID、状态）建立联合索引，查询速度提升80%。

②生成结果（图片/视频）：选择阿里云OSS（对象存储）。理由：图片/视频是大文件（图片几MB-几十MB，视频几十MB-几十GB），需支持高并发读写、分片上传/下载、CDN加速，OSS的扩展性好、存储成本低，且支持签名URL访问（保障数据安全）。优化：a. 数据压缩：图片用WebP格式（比JPG小30%），视频用H.265编码（比H.264小50%），降低存储成本和传输带宽；b. 分片上传：视频文件超100MB时，采用分片上传（每片5MB），避免传输失败；c. CDN加速：将OSS文件接入CDN，用户访问生成结果时，从就近CDN节点获取，加载速度从3秒降至500ms。

③热点数据缓存（如用户最近7天的生成结果、常用模型参数模板）：选择Redis。理由：热点数据需低延迟访问，Redis的内存读写速度快（10万+ QPS），支持多种数据结构（String/Hash/List）。优化：设置合理的过期策略（热点数据7天过期），采用LRU淘汰机制，避免缓存雪崩（通过Redis集群+主从复制）和缓存击穿（对热点key设置互斥锁）；将生成结果的URL缓存到Redis，用户查询时先查缓存，未命中再查OSS，查询延迟降低60%。

成本优化：将3个月以上的低频访问视频文件，从OSS标准存储迁移至归档存储（成本降低70%），需要时再解冻；通过Redis缓存减少OSS的访问次数，降低OSS的流量费用。”

##### 问题2：AIGC服务监控体系搭建+重点指标+工具+故障定位  
“监控体系采用「指标监控+日志分析+链路追踪」三位一体方案，确保服务可观测性：

①监控体系搭建：a. 指标采集：用Prometheus采集服务指标，通过Exporter（如node_exporter采集服务器资源、python-prometheus-client采集业务指标、nvidia-dcgm-exporter采集GPU指标）；b. 可视化：用Grafana搭建监控面板，按「全局概览→服务详情→GPU详情→存储详情」分层展示；c. 告警：用AlertManager配置告警规则，触发后通过钉钉/邮件通知运维和开发；d. 日志分析：用ELK Stack（Elasticsearch+Logstash+Kibana）收集结构化日志（JSON格式），支持按关键词检索、日志聚合分析；e. 链路追踪：用Jaeger，追踪用户请求从入口到模型调用、存储的全链路耗时。

②重点监控指标：a. 业务指标：QPS、响应延迟（P50/P95/P99）、任务成功率、任务失败率（按失败原因分类：模型调用失败、存储超时、参数错误）；b. 资源指标：CPU使用率、内存占用、GPU使用率、GPU显存占用、磁盘IO、网络带宽；c. 存储指标：OSS读写QPS、Redis缓存命中率、PostgreSQL查询延迟；d. 模型指标：模型推理延迟、模型调用成功率、批量处理效率。

③故障定位流程：以「生视频任务超时」为例，步骤如下：a. 查看Grafana面板，确认是整体延迟升高还是部分任务超时；b. 若整体延迟升高，查看GPU使用率（是否达100%）和队列长度（是否堆积），若GPU满载，说明并发过高，需扩容；c. 若部分任务超时，通过Jaeger查看链路耗时，定位是模型推理耗时久、存储写入慢还是GRPC调用超时；d. 若模型推理耗时久，查看TensorRT引擎是否正常，GPU显存是否泄漏；e. 若存储写入慢，查看OSS接口响应时间，通过Kibana检索存储相关日志，定位是否是网络问题或OSS服务异常；f. 最终根据定位结果，采取对应措施（如扩容GPU、优化模型推理、切换存储节点）。”


### 三、项目实战深挖（25分钟）
#### 1. 项目背景+业务目标+技术架构+核心模块  
“我最有代表性的项目是「企业级生视频智能生成平台」，服务于电商和广告行业客户，核心需求是：客户上传产品图片/文本描述，平台自动生成15-60秒的营销视频（如产品展示、促销活动），支持自定义风格（如科技风、文艺风）、背景音乐、字幕添加，要求支持高并发（日均10万+任务）、低延迟（生成时间≤10秒）、高可用（服务可用性≥99.9%）。

技术架构：采用「微服务+云原生」架构，分为5层：①接入层（Nginx+API Gateway）：负责请求转发、限流、认证；②业务层（Python+FastAPI）：负责任务接收、参数校验、业务逻辑处理；③调度层（Go+GRPC）：负责GPU资源调度、任务分发、优先级管理；④推理层（TensorRT+Diffusers）：负责生视频模型推理、视频后处理；⑤存储层（PostgreSQL+Redis+OSS）：负责元数据存储、缓存、大文件存储；⑥监控层（Prometheus+Grafana+ELK+Jaeger）：负责全链路监控和日志分析。

我负责的核心模块：①高并发任务调度模块：设计「协程+多进程」混合架构，处理请求接收和任务分发；②模型工程化模块：负责生视频模型的格式转换、TensorRT优化、推理接口封装；③存储方案设计模块：设计分层存储架构，优化读写性能和成本；④监控与运维模块：搭建全链路监控体系，负责线上故障排查和优化。”

#### 2. 最大技术挑战+分析+方案+落地效果  
“最大的技术挑战是「高并发场景下的低延迟与GPU资源高效利用」。初期上线时，遇到两个核心问题：①高并发请求导致任务排队严重，生成延迟超20秒，远高于目标的10秒；②GPU利用率低（仅30%-40%），但新增GPU成本高，资源浪费严重。

分析过程：①通过Prometheus监控发现，单GPU的并发任务数仅3个，导致GPU资源闲置；②通过Jaeger链路追踪发现，模型推理耗时占比80%，但推理过程中GPU计算资源未充分利用（Batch Size过小）；③任务调度采用「先来先服务」模式，未考虑任务优先级和GPU负载，导致高优先级任务排队。

解决方案：分3步优化：

①模型推理优化：a. 采用TensorRT FP16量化+算子融合，将模型推理延迟从8秒降至3.5秒；b. 实现动态批处理，根据GPU负载自动调整Batch Size（1-8），当GPU空闲时，合并多个任务批量推理，提升GPU利用率；c. 启用模型并行+张量并行，将大参数量模型拆分到多个GPU，单GPU显存占用降低35%，并发支持从3提升至10。

②任务调度优化：a. 引入优先级调度机制，将客户付费等级、任务紧急程度纳入优先级评分，高优先级任务优先分配GPU；b. 基于Go的GPM调度模型，开发GPU负载均衡算法，实时监控每个GPU的使用率、显存占用，将任务分配给负载最低的GPU；c. 优化任务队列，采用「本地队列+全局队列」双层结构，减少任务调度的网络开销。

③服务扩容优化：通过K8s HPA实现弹性扩容，当GPU利用率超70%或队列长度超500时，自动新增Pod；当负载降低时，自动缩容，避免资源浪费。

落地效果：①生成延迟从20秒降至4.5秒，满足业务目标（≤10秒）；②GPU利用率从35%提升至80%，无需新增GPU，资源成本降低30%；③服务QPS从8提升至35，支持日均15万+任务，远超初期目标的10万+；④任务成功率从95%提升至99.9%，服务可用性达标。”

#### 3. 与算法团队对接+协同优化  
“与算法团队的对接核心是「明确边界+高效协同」，具体流程：

①需求对齐：项目初期，组织算法、产品、后端三方会议，明确生视频模型的核心指标（如生成分辨率、风格支持范围、推理延迟上限）、输入输出格式（如prompt字段要求、视频输出格式）、交付物清单（模型文件、配置文件、依赖库、测试用例）。

②模型交付与验证：算法团队交付模型后，我会先在测试环境进行兼容性验证，重点检查：模型是否支持动态Batch Size、输入参数是否与业务需求一致、推理过程中是否有显存溢出。曾遇到算法交付的模型不支持动态分辨率，导致业务端无法满足客户自定义分辨率的需求，我们通过沟通，算法团队对模型进行修改，增加动态分辨率适配，后端同步调整推理接口的参数校验逻辑。

③协同优化：模型推理速度和显存占用是核心优化点，我们建立了「算法优化+工程优化」的协同机制：a. 算法层面：算法团队优化模型结构（如减少网络层数、采用轻量级 backbone），降低参数量；b. 工程层面：我负责模型量化、算子融合、并行策略优化。例如，针对模型显存占用过高的问题，算法团队通过知识蒸馏生成轻量模型（参数量减少50%），我再通过TensorRT FP16量化，将显存占用从16GB降至4GB，单GPU并发支持从1提升至10。

④问题反馈闭环：建立即时沟通群，后端遇到模型相关问题（如推理失败、生成效果异常），第一时间反馈给算法团队，附上详细日志（如输入参数、错误信息、GPU状态）；算法团队优化后，及时同步给后端进行验证，确保问题快速解决。”

#### 4. 线上故障+应急处理+复盘优化  
“印象最深的一次线上故障是「生视频任务失败率骤升（从0.1%升至15%）」，发生在电商大促期间（QPS是平时的3倍）。

应急处理：①紧急止血（0-30分钟）：通过监控发现失败原因集中在「OSS存储写入超时」，立即切换至备用OSS存储节点，同时暂停低优先级任务（普通用户），优先保障高优先级任务（付费用户），失败率快速降至2%；②临时扩容（30分钟-2小时）：新增2个GPU节点和1个OSS存储节点，提升并发处理能力和存储读写速度，失败率降至0.5%以下；③故障排查（2-4小时）：通过Kibana检索日志，发现是大促期间OSS主节点流量过载，导致写入超时，同时后端服务的OSS连接池配置过小（仅10个连接），无法满足高并发写入需求。

复盘优化：①短期优化：调整OSS连接池大小（从10增至50），启用连接池复用，同时配置OSS多节点负载均衡，避免单点过载；②长期优化：a. 存储层降级策略：当OSS写入超时，自动将生成结果暂存至本地磁盘，后续异步同步至OSS，保障任务成功率；b. 监控优化：新增OSS连接池使用率、存储节点响应时间的监控指标，设置告警阈值（如响应时间超500ms触发告警）；c. 容量规划：大促前1个月，基于历史数据预测流量（如预计QPS是平时的3倍），提前扩容GPU和存储资源，进行压力测试，确保服务能支撑峰值流量。

优化后效果：后续多次电商大促，生视频任务失败率稳定在0.1%以下，未再出现类似故障。”

#### 5. 项目重新设计的改进点  
“如果重新设计这个项目，我会在3个方面改进：

①架构层面：将业务层和推理层进一步解耦，采用「消息队列+微服务」架构，业务层接收请求后，将任务消息发送至Kafka，推理层通过消费Kafka消息处理任务，避免业务层和推理层直接耦合，提升系统的扩展性（如新增推理模型时，无需修改业务层代码）。

②模型管理层面：引入模型仓库（如MLflow），统一管理模型版本、迭代记录、性能指标，支持模型的灰度发布和回滚（当前模型迭代后，若效果不佳，回滚成本高）；同时实现模型的自动更新，当算法团队上传新模型后，系统自动进行兼容性测试，测试通过后灰度发布，提升模型迭代效率。

③成本优化层面：引入GPU分时调度机制，针对非峰值时段（如凌晨1-6点），将空闲GPU资源用于模型微调、数据预处理等离线任务，提升GPU资源的利用率（当前非峰值时段GPU利用率仅20%）；同时采用云原生的Serverless架构，针对轻量任务（如低分辨率生图），使用Serverless GPU，按使用时长计费，降低闲置成本。”


### 四、场景题+压力测试（15分钟）
#### 场景题1：高并发应急（QPS从100突增至1000，服务超时+任务失败）  
“我会按「紧急止血→短期优化→长期架构升级」三个层面处理：

①紧急止血（0-1小时）：a. 限流降级：通过Nginx紧急提高限流阈值（从100QPS升至500QPS），同时对普通用户实施降级（如生成分辨率从1080P降至720P），保障付费用户的服务质量；b. 队列扩容：临时新增Redis节点，扩容Celery任务队列，避免队列堆积导致的任务丢失；c. 资源扩容：快速新增5个GPU节点（基于K8s弹性扩容），提升并发处理能力；d. 故障隔离：将超时任务自动转移至备用服务集群，避免影响正常任务处理。

②短期优化（1-24小时）：a. 任务调度优化：调整Celery的任务优先级策略，付费用户任务优先级提升至最高，优先处理；b. 缓存优化：扩大Redis缓存范围，将常用生成参数模板、用户历史配置缓存至Redis，减少数据库查询耗时；c. 模型推理优化：临时关闭部分非核心后处理功能（如字幕自动生成、背景音乐自定义），缩短生成链路；d. 监控强化：新增队列长度、任务超时率的实时告警，安排专人值守，及时处理突发问题。

③长期架构升级（1-7天）：a. 架构重构：采用「微服务+分布式任务调度」架构，将任务接收、调度、推理、存储拆分为独立微服务，提升扩展性；b. 弹性扩容：基于K8s HPA配置更精细的扩容策略（如QPS每增加100，新增2个GPU节点），支持秒级扩容；c. 流量削峰：引入消息队列（Kafka），将突发流量缓冲至队列，避免直接冲击推理层；d. 多区域部署：将服务部署到多个地域，通过CDN和负载均衡实现流量分担，提升容灾能力。”

#### 场景题2：生视频服务优化（Python+FastAPI，单GPU延迟5秒，QPS10，GPU利用率30%）  
“具体优化方向和实施步骤，附预期效果：

①模型推理优化（核心）：a. 格式转换+TensorRT优化：将PyTorch模型转ONNX，再通过TensorRT FP16量化+算子融合，预期推理延迟从5秒降至1.5秒（降低70%）；b. 动态批处理：实现Batch Size自适应调整（1-8），当有多个任务时，合并批量推理，预期GPU利用率从30%提升至75%；c. 模型并行：将生视频模型的编码器和解码器拆分到2个GPU，单GPU显存占用降低50%，并发支持提升1倍。

②服务架构优化：a. 异步化改造：将模型调用后的存储写入、结果回调改为asyncio异步操作，避免IO阻塞，预期单请求处理耗时减少1秒；b. 任务队列优化：用Celery+Redis替代内置队列，支持任务优先级和重试，同时配置Worker进程数为GPU核心数的2倍，提升任务处理并行度，预期QPS从10提升至30。

③资源调度优化：a. GPU负载均衡：引入Go开发的GPU调度模块，实时监控GPU利用率，将任务分配给负载最低的GPU，避免单GPU过载；b. 显存优化：关闭模型的gradient checkpointing，手动释放中间张量，预期单任务显存占用降低20%，支持更多并发。

④预期整体效果：延迟从5秒降至2秒以内，QPS从10提升至35，GPU利用率从30%提升至80%，完全满足业务需求。”

#### 场景题3：跨团队协同（新模型显存占用16GB，推理10秒，需提升至5个并发）  
“我会分三步与算法团队、产品团队协同解决：

①需求对齐与问题分析（1天内）：a. 组织三方会议，明确核心目标：在不降低生成效果的前提下，将单GPU并发从1提升至5，即单任务显存占用≤3.2GB，推理延迟≤10秒；b. 分析瓶颈：当前模型显存占用16GB（单任务），推理10秒，核心问题是模型参数量大、推理并行度低。

②与算法团队协同优化（1-2周）：a. 模型轻量化：算法团队通过知识蒸馏、剪枝等手段，减少模型参数量（如从10B降至5B），预期显存占用降低40%；b. 精度优化：算法团队支持FP16量化，同时保证生成效果无明显下降，预期显存占用再降低50%；c. 推理优化：算法团队优化模型推理流程，减少冗余计算（如共享特征图），预期推理延迟从10秒降至8秒。

③与产品团队协同平衡（同步进行）：a. 功能降级协商：若算法优化后仍无法满足并发需求，与产品团队协商短期降级方案（如非核心场景生成分辨率从1080P降至720P，预期显存占用再降低30%）；b. 优先级排序：产品团队梳理业务场景，优先保障核心场景（如电商大促视频生成）的并发需求，非核心场景可适当降低并发标准；c. 长期规划：产品团队同步规划资源扩容预算，后续新增GPU节点，从根本上解决并发瓶颈。

④预期结果：通过算法优化+功能降级，单任务显存占用降至3GB以内，推理延迟8秒，单GPU并发支持提升至5，满足产品要求；同时制定长期扩容计划，保障业务后续增长。”


### 五、软技能+反问环节（10分钟）
#### 1. 选择AIGC后端方向的原因+学习方式+近期学习内容  
“选择AIGC后端方向，一是因为AIGC是当前技术热点，生图/生视频等场景的商业化落地需求旺盛，有广阔的职业发展空间；二是我本身擅长后端开发，同时对AI技术感兴趣，AIGC后端能将后端架构能力与AI模型工程化结合，发挥我的优势。

学习方式主要有3种：①项目实战：在工作中遇到问题后，深入研究底层原理，比如模型推理加速、跨语言通信，通过解决实际问题巩固知识；②技术社区：关注GitHub、知乎、InfoQ，学习行业大牛的技术分享，参与开源项目（如Diffusers的Issues讨论）；③系统学习：报名极客时间的「AIGC工程化实战」「Go底层原理剖析」等课程，系统梳理知识体系；④团队交流：定期与算法团队、其他后端同事组织技术分享，碰撞思路。

近期学习的内容：①大模型推理框架vLLM，重点研究其PagedAttention机制，尝试将其集成到现有生视频服务中，进一步提升推理并发度；②Go语言的分布式调度框架（如Kratos），学习其高可用设计，用于优化现有Go调度模块；③AIGC安全与合规相关知识（如生成内容审核、数据加密），应对业务合规需求。”

#### 2. 跨团队协作+意见分歧解决  
“之前的团队中，我主要通过「明确边界+有效沟通+结果导向」与算法、前端、产品团队协作：①明确边界：每个团队的职责的提前对齐，比如算法团队负责模型效果和性能，后端负责模型工程化和服务稳定性，避免职责模糊导致的推诿；②有效沟通：建立定期沟通机制（如每日站会、每周复盘会），及时同步进度和问题；对技术细节，用文档（如接口文档、技术方案）明确，避免口头沟通的歧义；③结果导向：所有协作都以业务目标为核心，比如生视频模型的优化，最终以「延迟降低多少、并发提升多少」为衡量标准。

遇到过的意见分歧：在生视频模型的量化方案上，算法团队担心FP16量化会影响生成效果，希望用FP32精度，而后端团队需要降低显存占用，提升并发。解决方式：①数据验证：我搭建了测试环境，用FP16和FP32分别测试1000个样本，对比生成效果（邀请产品团队、算法团队共同评审）和性能数据；②结果对齐：测试结果显示，FP16量化后生成效果无明显差异，但显存占用降低50%，并发提升1倍；③折中方案：最终采用FP16量化，同时算法团队优化模型的量化策略，确保边缘场景的生成效果，双方达成共识。”

#### 3. 1-3年职业规划+希望在岗位上获得的成长  
“1-3年的职业规划：①短期（1年）：快速融入团队，熟悉公司的AIGC业务场景和技术架构，独立负责核心模块的开发和优化，解决实际业务问题；②中期（2年）：成为AIGC后端领域的技术专家，深入研究大模型工程化、高并发架构设计，主导复杂项目的技术方案设计，带领小团队完成项目落地；③长期（3年）：参与公司AIGC技术体系的搭建，推动技术创新（如多模态模型工程化、AI Agent与AIGC结合），成为技术负责人，引领团队技术方向。

希望在岗位上获得的成长：①技术深度：在AIGC模型工程化、高并发服务优化等核心领域，获得更深入的实践经验，尤其是生视频等复杂场景的技术难题解决；②业务视野：了解公司AIGC业务的商业化逻辑，参与从技术到产品的落地全流程，提升技术商业化思维；③团队协作：与更优秀的算法、产品、前端团队协作，学习他们的思维方式，提升跨团队协作和项目管理能力。”

#### 4. 反问环节  
“感谢面试官！我有3个问题想请教：①请问公司的AIGC业务主要聚焦哪些场景？生图/生视频的核心用户是To B还是To C？②团队当前的技术栈中，Go模块的占比如何？后续是否有计划进一步扩大Go在底层架构中的应用？③这个岗位入职后，首先会负责什么类型的工作？是否有明确的项目或任务优先级？”